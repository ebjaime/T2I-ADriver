{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8c7845-0931-405a-974f-04aa38d2c212",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c03ceec-f271-49fe-81fa-e0f6ea8eaba2",
   "metadata": {},
   "source": [
    "First, generate prompts with LLAVA and BLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cd8d25-44af-4f65-ad73-8c6c8fcdbb38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python T2I-Adriver/llava_dataset_creation.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e449b2f4-6878-425c-98cf-33f6f5c45c07",
   "metadata": {},
   "source": [
    "## Other models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705c9743-79dd-41f9-846c-06566fce4c0a",
   "metadata": {},
   "source": [
    "Run SD, SDI2I, ControlNet and T2I with each prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d0c0b7e-5faa-49e0-a547-d21a85195462",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from diffusers import StableDiffusionPipeline, StableDiffusionImg2ImgPipeline, T2IAdapter, EulerDiscreteScheduler, ControlNetModel\n",
    "import torch\n",
    "import os\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "from diffusers import T2IAdapter, StableDiffusionAdapterPipeline\n",
    "from diffusers.utils import load_image\n",
    "import ipyplot\n",
    "from utils_controlnet import reshape, to_canny, to_depth, to_segmentation\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# INPUTS\n",
    "MODEL = \"SD\"  # Options: \"SD\", \"SD Image to Image\", \"T2I\", \"ControlNet\"\n",
    "PROMPT = \"Manual_Entry\"\n",
    "#FILE = pd.read_csv(\"T2I-Adapter/dataset/other_models/llava_long_prompts.csv\") if PROMPT == \"LLAVA\" else pd.read_csv(\"T2I-Adapter/dataset/other_models/blip_prompts.csv\") if PROMPT == \"BLIP\" else pd.read_csv(\"T2I-Adapter/dataset/other_models/manual_prompts.csv\")\n",
    "FILE = pd.read_csv(\"T2I-Adriver/dataset/val_short_prompts.csv\")\n",
    "PROMPTS = FILE[\"Labels\"]\n",
    "IMAGE_PATHS = \"T2I-Adriver/dataset/val_6/\"+FILE[\"Image\"]\n",
    "generator = torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd480a4f-d875-4e87-9ab3-17aece3ba145",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "color = cv2.imread(\"cloudy_road.png\")\n",
    "color = cv2.cvtColor(color, cv2.COLOR_BGR2RGB)  # Convert from BGR to RGB\n",
    "\n",
    "# Resize down and up using specified interpolation methods\n",
    "color = cv2.resize(color, (512 // 64, 512 // 64), interpolation=cv2.INTER_CUBIC)\n",
    "color = cv2.resize(color, (512, 512), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "# Convert back to PIL Image and store\n",
    "color_image = Image.fromarray(color)\n",
    "#color_images.append(color_image)\n",
    "color_image.save(\"cloudy_road_color.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b383a254-986e-421e-841f-ebef130b655b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_inference_steps = 50  # Modify as needed to change the noise schedule\n",
    "guidance_scale = 7.5  # Modify to control the influence of the prompt\n",
    "\n",
    "output_dir = \"T2I-Adriver/dataset/other_models/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Model mapping based on the variable\n",
    "# Models\n",
    "sd14 = \"CompVis/stable-diffusion-v1-4\"\n",
    "sd15 = \"runwayml/stable-diffusion-v1-5\"\n",
    "sdxl = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "\n",
    "# T2I SD 1.4/1.5 checkpoints\n",
    "canny_t2i = \"TencentARC/t2iadapter_canny_sd15v2\"\n",
    "depth_t2i = \"TencentARC/t2iadapter_depth_sd15v2\" \n",
    "sketch_t2i = \"TencentARC/t2iadapter_sketch_sd15v2\"\n",
    "segmentation_t2i = \"TencentARC/t2iadapter_seg_sd14v1\"\n",
    "zoedepth_t2i = \"TencentARC/t2iadapter_zoedepth_sd15v1\"\n",
    "openpose_t2i = \"TencentARC/t2iadapter_openpose_sd14v1\"\n",
    "color_t2i = \"TencentARC/t2iadapter_color_sd14v1\"\n",
    "\n",
    "# ControlNet checkpoints\n",
    "canny_cn = \"lllyasviel/sd-controlnet-canny\"\n",
    "depth_cn = \"lllyasviel/control_v11f1p_sd15_depth\"\n",
    "segmentation_cn = \"lllyasviel/control_v11f1p_sd15_depth\"\n",
    "openpose_cn = \"lllyasviel/sd-controlnet-openpose\"\n",
    "#sketch_cn = \"lllyasviel/control_sd15_sketch\"\n",
    "\n",
    "# Create conditionings\n",
    "# Load images \n",
    "init_images = [Image.open(image_path).convert(\"RGB\").resize((512, 512)) for image_path in IMAGE_PATHS]\n",
    "\n",
    "canny_images=[]\n",
    "for i, canny_image in enumerate(IMAGE_PATHS):\n",
    "    canny_image = to_canny(canny_image)\n",
    "    canny_image = reshape(canny_image,(512,512))\n",
    "    canny_images.append(canny_image)\n",
    "    \n",
    "depth_images=[]\n",
    "for i, depth_image in enumerate(IMAGE_PATHS):\n",
    "    depth_image = to_depth(depth_image)\n",
    "    depth_image = reshape(depth_image,(512,512))\n",
    "    depth_images.append(depth_image)\n",
    "    \n",
    "segmentation_images=[]\n",
    "for i, segmentation_image in enumerate(IMAGE_PATHS):\n",
    "    segmentation_image = to_segmentation(segmentation_image)\n",
    "    segmentation_image = reshape(segmentation_image,(512,512))\n",
    "    segmentation_images.append(segmentation_image)\n",
    "    \n",
    "color_images = []\n",
    "for i, image_path in enumerate(IMAGE_PATHS):\n",
    "    # Load image\n",
    "    color = cv2.imread(image_path)\n",
    "    color = cv2.cvtColor(color, cv2.COLOR_BGR2RGB)  # Convert from BGR to RGB\n",
    "\n",
    "    # Resize down and up using specified interpolation methods\n",
    "    color = cv2.resize(color, (512 // 64, 512 // 64), interpolation=cv2.INTER_CUBIC)\n",
    "    color = cv2.resize(color, (512, 512), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    # Convert back to PIL Image and store\n",
    "    color_image = Image.fromarray(color)\n",
    "    color_images.append(color_image)\n",
    "    \n",
    "ipyplot.plot_images(color_images, max_images=20, img_width=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bee23c-6e95-442c-b70e-94da927ea23c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define a function to create filenames\n",
    "def create_filename(model_name, prompt, conditionings, index, guidance_scale):\n",
    "    # Create a sanitized version of the prompt to use in the filename\n",
    "    sanitized_prompt = prompt.replace(' ', '_').replace('/', '_').replace('\\\\', '_')\n",
    "    # Create a filename with model name, prompt, and conditionings\n",
    "    filename = f\"{model_name}_{sanitized_prompt}_{'_'.join((conditionings))}_{guidance_scale}_{index+1}.png\"\n",
    "    return filename\n",
    "\n",
    "# Model definition and generation of images\n",
    "MODEL = \"T2I\"\n",
    "guidance_scales = [7.5]\n",
    "for guidance_scale in guidance_scales:\n",
    "    if MODEL == \"T2I\":\n",
    "        from diffusers import StableDiffusionAdapterPipeline, T2IAdapter\n",
    "        import torch\n",
    "\n",
    "        # Load the different T2I Adapter models\n",
    "        adapter_sketch = T2IAdapter.from_pretrained(sketch_t2i, torch_dtype=torch.float16)\n",
    "        #adapter_color = T2IAdapter.from_pretrained(color_t2i, torch_dtype=torch.float16)\n",
    "        adapter_segmentation = T2IAdapter.from_pretrained(segmentation_t2i, torch_dtype=torch.float16)\n",
    "        adapter_depth = T2IAdapter.from_pretrained(depth_t2i, torch_dtype=torch.float16)\n",
    "\n",
    "        # Combine the adapters in a list\n",
    "        adapters = [adapter_sketch, adapter_segmentation, adapter_depth]\n",
    "\n",
    "        # Load the pipeline with all the adapters\n",
    "        pipe = StableDiffusionAdapterPipeline.from_pretrained(\n",
    "            sd14,\n",
    "            adapter=adapters,\n",
    "            torch_dtype=torch.float16,\n",
    "        )\n",
    "        pipe.to(\"cuda\")\n",
    "\n",
    "        t2i_images = []\n",
    "        for i, prompt in enumerate(PROMPTS):\n",
    "            # Define conditioning scales for each adapter\n",
    "            adapter_scales = [1.0, 1.0, 1.0]  # Adjust these values as needed\n",
    "\n",
    "            # Pass the conditioning images and corresponding scales\n",
    "            generated_image = pipe(\n",
    "                prompt,\n",
    "                image=[canny_images[1], segmentation_images[1], depth_images[1]],  # Order matches the adapters list\n",
    "                #generator=generator,\n",
    "                num_inference_steps=num_inference_steps,\n",
    "                negative_prompt=\"\",\n",
    "                adapter_conditioning_scale=adapter_scales,  # Pass the list of scales\n",
    "                #guidance_scale=guidance_scale\n",
    "            ).images[0]\n",
    "\n",
    "            t2i_images.append(generated_image)\n",
    "\n",
    "            #filename = create_filename(MODEL, PROMPT, [\"color\", \"depth\", \"seg\"], i, guidance_scale)\n",
    "            #file_path = os.path.join(output_dir, filename)\n",
    "            generated_image.save(f\"T2I_{i}_{guidance_scale}.png\")\n",
    "        canny_images[1].save(f\"T2I_sketch.png\")\n",
    "        segmentation_images[1].save(f\"T2I_seg.png\")\n",
    "        depth_images[1].save(f\"T2I_depth.png\")\n",
    "        ipyplot.plot_images(t2i_images, max_images=20, img_width=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3029387-bc9d-424e-bb09-84b2d30888fa",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ipyplot.plot_images(init_images, max_images=20, img_width=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1c19879b-722d-4611-a578-c849e3bd959a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#ipyplot.plot_images(segmentation_images, max_images=20, img_width=200)\n",
    "color_images[11].save(\"road_t2i_1_color.png\")\n",
    "canny_images[11].save(\"road_t2i_1_sketch.png\")\n",
    "init_images[11].save(\"road_t2i_1.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc9d911-984a-41ee-99a8-4c76118ea6ac",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL = \"ControlNet\"\n",
    "guidance_scales = [0,0.5,1,1.5, 5, 10]\n",
    "for guidance_scale in guidance_scales:\n",
    "    if MODEL == \"ControlNet\":\n",
    "        from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\n",
    "\n",
    "        # Load the different ControlNet models and move them to GPU\n",
    "        controlnet_sketch = ControlNetModel.from_pretrained(canny_cn, torch_dtype=torch.float16).to(\"cuda\")\n",
    "        controlnet_segmentation = ControlNetModel.from_pretrained(segmentation_cn, torch_dtype=torch.float16).to(\"cuda\")\n",
    "        controlnet_depth = ControlNetModel.from_pretrained(depth_cn, torch_dtype=torch.float16).to(\"cuda\")\n",
    "\n",
    "        # Combine the ControlNet models in a list\n",
    "        controlnets = [controlnet_sketch, controlnet_segmentation] #, controlnet_depth]\n",
    "\n",
    "        # Load the pipeline with all the ControlNet models and move to GPU\n",
    "        pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "            sd14, controlnet=controlnets, torch_dtype=torch.float16,\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "        pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "        pipe.enable_xformers_memory_efficient_attention()\n",
    "        pipe.enable_model_cpu_offload()\n",
    "\n",
    "        cn_images = []\n",
    "        for i, prompt in enumerate(PROMPTS):\n",
    "            # Pass the conditioning images in the same order as the ControlNet models\n",
    "            cn_images.append(pipe(\n",
    "                prompt,\n",
    "                image=[canny_images[i], segmentation_images[i]],#, depth_images[i]],  # Order should match the controlnets list\n",
    "                num_inference_steps=num_inference_steps,\n",
    "                generator=generator,\n",
    "                negative_prompt=\"\",\n",
    "                guidance_scale = guidance_scale\n",
    "            ).images[0])\n",
    "\n",
    "            #filename = create_filename(MODEL, PROMPT, [\"sketch\", \"depth\", \"seg\"], i, guidance_scale)\n",
    "            #file_path = os.path.join(output_dir, filename)\n",
    "            #cn_images[-1].save(file_path)\n",
    "            cn_images[-1].save(f\"ControlNetFail_test_{i}_{guidance_scale}.png\")\n",
    "\n",
    "        ipyplot.plot_images(cn_images, max_images=20, img_width=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e33671-6e3f-4d9f-bedc-eeb7990cdb69",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL = \"SD\"\n",
    "PROMPTS = [\"a sunset over the mountains\"]\n",
    "for guidance_scale in [5, 10]:\n",
    "    if MODEL == \"SD\":\n",
    "        from diffusers import StableDiffusionPipeline, UniPCMultistepScheduler\n",
    "\n",
    "        pipe = StableDiffusionPipeline.from_pretrained(\n",
    "            sd14,  torch_dtype=torch.float16,\n",
    "        )\n",
    "        pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "        pipe.to(\"cuda\")  \n",
    "        sd_images = []\n",
    "        for i, prompt in enumerate(PROMPTS):\n",
    "            sd_images.append(pipe(\n",
    "                prompt,\n",
    "                generator=generator,\n",
    "                num_inference_steps=num_inference_steps,\n",
    "                negative_prompt=\"\",  # Optionally include negative prompts if needed\n",
    "                guidance_scale=guidance_scale\n",
    "            ).images[0])\n",
    "\n",
    "            filename = create_filename(MODEL, PROMPT, [\"\"], i, \"_\")\n",
    "            file_path = os.path.join(output_dir, filename)\n",
    "            sd_images[-1].save(f\"sunset_{guidance_scale}.png\")\n",
    "\n",
    "        ipyplot.plot_images(sd_images, max_images=20, img_width=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d990e119-cbf2-48a9-a0df-2e23f620cce6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL = \"SDI2I\"\n",
    "for guidance_scale in guidance_scales:\n",
    "    if MODEL == \"SDI2I\":\n",
    "        from diffusers import StableDiffusionImg2ImgPipeline\n",
    "        import torch\n",
    "\n",
    "        # Load the Stable Diffusion Image-to-Image pipeline\n",
    "        pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
    "            sd14,  # Replace with your desired Stable Diffusion model version\n",
    "            torch_dtype=torch.float16,\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "        pipe.enable_xformers_memory_efficient_attention()\n",
    "\n",
    "        sdi2i_images = []\n",
    "\n",
    "        for i, prompt in enumerate(PROMPTS):\n",
    "            if i != 6:\n",
    "                continue\n",
    "            # Generate the image based on the corresponding prompt and initial image\n",
    "            generated_image = pipe(\n",
    "                prompt=prompt,\n",
    "                image=init_images[i],  # Use the preloaded image\n",
    "                strength=guidance_scale/10,  # Adjust the strength of the transformation (0.0-1.0)\n",
    "                guidance_scale=guidance_scale,  # Adjust the guidance scale (how closely the image should match the prompt)\n",
    "                generator=torch.manual_seed(42),  # Optional: Set a seed for reproducibility\n",
    "                num_inference_steps=num_inference_steps,\n",
    "                negative_prompt=\"\"  # Optionally include a negative prompt\n",
    "            ).images[0]\n",
    "\n",
    "            sdi2i_images.append(generated_image)\n",
    "\n",
    "            filename = create_filename(MODEL, PROMPT, [\"\"], i, guidance_scale)\n",
    "            file_path = os.path.join(output_dir, filename)\n",
    "            generated_image.save(file_path)\n",
    "\n",
    "        ipyplot.plot_images(sdi2i_images, max_images=20, img_width=200)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
